{"cells":[{"cell_type":"markdown","source":["# INTRO\n","This notebook contains experiments for:\n","\n","\n","1.   Proposed model - **Text-to-graph + Aggregate rule**\n","2.   Baseline models:\n","  \n","  2.1 Models - **SGDClassifier, KNeighborsClassifier, GaussianNB**\n","\n","  2.2 Feature engeneering - **word n-gram, Word2Vec, FastText, LIWC**\n","\n","To run part additional model for amrlib must be dowloaded and unarchived - https://github.com/bjascob/amrlib-models/releases/download/parse_xfm_bart_base-v0_1_0/model_parse_xfm_bart_base-v0_1_0.tar.gz\n","\n","To execute this code faster, the use of GPU on  is recommended.\n","\n"],"metadata":{"id":"DjDN7wSh4JbM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r84c1ycpaMTB"},"outputs":[],"source":["!pip install amrlib\n","!wget https://github.com/bjascob/amrlib-models/releases/download/parse_xfm_bart_base-v0_1_0/model_parse_xfm_bart_base-v0_1_0.tar.gz\n","!tar xzf model_parse_xfm_bart_base-v0_1_0.tar.gz\n","!pip install Unidecode\n","!pip install graphviz\n","!pip install emoji\n","!pip install liwc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33938,"status":"ok","timestamp":1735287149054,"user":{"displayName":"Анна Назарова","userId":"17138035556691584905"},"user_tz":-180},"id":"iVvbFqchabAy","outputId":"d5575d5d-6f96-480a-fbb0-5610fe7317af"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import random\n","import pickle\n","import amrlib\n","import re\n","import nltk\n","import penman\n","import unicodedata\n","from tqdm import tqdm\n","import string\n","import emoji\n","import graphviz\n","import logging\n","import gensim\n","import spacy\n","import warnings\n","import liwc\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from gensim import corpora\n","\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","from collections import Counter\n","\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","stop_eng = stopwords.words('english')\n","\n","warnings.filterwarnings(\"ignore\")\n","logging.disable(level=logging.CRITICAL)\n","\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slvjZy17affT"},"outputs":[],"source":["df = pd.read_excel(\"Threat_translated_into_English.xlsx\")\n","df = df.drop([\"Tweets\", \"Unnamed: 0\"], axis=1)\n","df = df.rename(columns={\"Translated\": \"Text\", \"label\": \"Label\"})\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"dSbHVpWCPr3Q"},"source":["# Text-to-graph + Aggregate rule"]},{"cell_type":"markdown","source":["## Data preparatin\n","\n","In this section exmaples from dataset are prepared to be used in gSofia:\n","\n","\n","1.   Examples are devided into sentences\n","2.   For each sentence an AMR graph is build\n","3.   All instance entities and related links are removed from the graph. The names of all internal vertices are replaced with their corresponding (names used in) instance entities.\n","4.   Lemmatizing and lowering word in nodes.\n","5.   Merging all AMR graph of the tweet into one.\n","\n"],"metadata":{"id":"pE8_ANBOCoTp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8_6IdCxKeKXg"},"outputs":[],"source":["stog = amrlib.load_stog_model(model_dir=\"/content/model_parse_xfm_bart_base-v0_1_0\",)\n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgDKX5vHamRG"},"outputs":[],"source":["\"\"\"\n","A method that runs through all the sentences\n","from the dataset and executes the DocToGraph from the paper\n","\"\"\"\n","def enterPoint(df):\n","    res = []\n","    for text in tqdm(df['Text']):\n","        res.append(DocToGraph(text))\n","    return res\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtLBaHEaaoxG"},"outputs":[],"source":["\"\"\"\n","Makes all modifications to create the desired AMR graph\n","\"\"\"\n","def DocToGraph(text):\n","    r = []\n","    text = text.replace(\"\\n\", \" \")\n","    sent = text.split(\".\")\n","    for s in sent:\n","        if len(s)==0:\n","                continue\n","        a = sentenceToAMRGraph(s)\n","        m = ModifyGraph(a)\n","        r.append(refineGraph(m))\n","    g = mergeGraphs(r)\n","    return g"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5SbnPgtaqU_"},"outputs":[],"source":["\"\"\"\n","Translates a single sentence into a standard AMR graph\n","\"\"\"\n","def sentenceToAMRGraph(s):\n","    arr = [s]\n","    graphs = stog.parse_sents([s])\n","    return graphs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfnUtzBzgf0k"},"outputs":[],"source":["\"\"\"\n","Lemmatisation of words in the graph\n","Lowercase words\n","\"\"\"\n","def refineGraph(m):\n","  for el in m:\n","    el[0] = lemmatizer.lemmatize(el[0].lower())\n","    el[2] = lemmatizer.lemmatize(el[2].lower())\n","\n","  return m"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YWjusCjhKkr"},"outputs":[],"source":["def mergeGraphs(r):\n","  new_graph = []\n","  for graph in r:\n","    for el in graph:\n","      if el not in new_graph:\n","        new_graph.append(el)\n","\n","  return new_graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iehVypFCbQ4s"},"outputs":[],"source":["\"\"\"\n","Modifies the graph:\n","1. Finds all duplicates of a single object\n","2. Replaces duplicates with the original one\n","Example, orignal -\n",":ARG0 (p / person) # 3\n","                  :ARG1 (p2 / person # 4\n","                        :mod (h / hypocrite)) # 5\n","                  :manner (l / live-01 # 6\n","                        :ARG0 p2)\n","after modification -\n",":ARG0 (person) # 3\n","                  :ARG1 (person2 # 4\n","                        :mod (hypocrite)) # 5\n","                  :manner (live-01 # 6\n","                        :ARG0 person2)\n","\n","\"\"\"\n","def ModifyGraph(a):\n","  g = penman.decode(a)\n","  arr_trip = g.triples\n","  dict_inst = dict()\n","  new_graph = []\n","\n","  for el in arr_trip:\n","    if ':instance' in el:\n","      dict_inst[el[0]] = el[2]\n","  for el in arr_trip:\n","      if ':instance' not in el:\n","        try:\n","          new_graph.append([dict_inst[el[0]], el[1], dict_inst[el[2]]])\n","        except KeyError:\n","          if el[0] not in dict_inst:\n","            dict_inst[el[0]] = el[0]\n","          if el[2] not in dict_inst:\n","            dict_inst[el[2]] = el[2]\n","          new_graph.append([dict_inst[el[0]], el[1], dict_inst[el[2]]])\n","  return new_graph\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDL0M4aEhYpM"},"outputs":[],"source":["def save_unique_id(dict_num_all):\n","  with open('dict_vocab.txt', 'wb') as fp:\n","    pickle.dump(dict_num_all, fp)\n","    print('dictionary saved successfully to file')\n","\n","def save_graph(fin_str, name):\n","  with open(name, 'w') as text_file:\n","    text_file.write(fin_str)\n","    print('Graphs saved successfully to file')\n","\n","\"\"\"\n","all_g - array of all graphs threat or not-trea separately\n","dict_num_all - dict of unique id for elements in all graphs\n","i - unique number for all words in all graphs\n","\"\"\"\n","def prepareForGSofia(all_g, dict_num_all, i):\n","  graph_num=1\n","  fin_str = \"\"\n","\n","  for g in all_g: # run all graphs\n","    fin_str = fin_str + \"t # \"+str(graph_num)+\"\\n\"\n","    graph_num+=1\n","    dict_num = {} # unique id for elements in one graphs\n","    j = 0\n","    graph_str = \"\"\n","\n","    for el in g: #get all vertices\n","      if el[0] not in dict_num: # number for source inside the graph for all in one graph\n","        dict_num[el[0]] = j\n","        j+=1\n","      if el[0] not in dict_num_all:\n","        dict_num_all[el[0]] = i\n","        i+=1\n","\n","\n","      if el[2] not in dict_num: # number for target inside the graph for all or one graph\n","        dict_num[el[2]] = j\n","        j+=1\n","      if el[2] not in dict_num_all:\n","        dict_num_all[el[2]] = i\n","        i+=1\n","\n","      if el[1] not in dict_num_all:# number for type inside the graph for all graphs\n","        dict_num_all[el[1]] = i\n","        i+=1\n","\n","      str_edge = \"v \"+ str(dict_num[el[0]]) +\" \"+ str(dict_num_all[el[0]])+\"\\n\"\n","      if str_edge not in graph_str:\n","        graph_str=graph_str+str_edge\n","      str_edge = \"v \"+ str(dict_num[el[2]]) + \" \"+ str(dict_num_all[el[2]])+\"\\n\"\n","      if str_edge not in graph_str:\n","        graph_str=graph_str+str_edge\n","\n","    fin_str=fin_str+graph_str\n","\n","    for el in g: # run all edges\n","      str_edge = \"e \" +str(dict_num[el[0]]) +\" \"+str(dict_num[el[2]])+\" \"+str(dict_num_all[el[1]])+\"\\n\"\n","      fin_str=fin_str+str_edge\n","\n","  return fin_str, dict_num_all, i\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIdIZg3ivMKy"},"outputs":[],"source":["#Create a train:test split\n","split=StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n","X = df\n","y =  df['Label']\n","X_train, y_train = None, None\n","X_test, y_test = None, None\n","for i, (train_index, test_index) in enumerate(split.split( X, y)):\n","     X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n","     X_test, y_test = X.iloc[test_index], y.iloc[test_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqnDywn3vf1q"},"outputs":[],"source":["dict_num_all = {}\n","i = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEoDgexAvW6w"},"outputs":[],"source":["threat_speech_train = enterPoint(X_train[X_train['Label']==1])\n","fin_str, dict_num_all, i = prepareForGSofia(threat_speech_train, dict_num_all, i)\n","save_graph(fin_str, \"threat_train_90.txt\")\n","\n","threat_speech_test = enterPoint(X_test[X_test['Label']==1])\n","fin_str, dict_num_all, i = prepareForGSofia(threat_speech_test, dict_num_all, i)\n","save_graph(fin_str, \"threat_test_90.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTo9Ua5LvnMs"},"outputs":[],"source":["non_threat_speech_train = enterPoint(X_train[X_train['Label']==0])\n","fin_str, dict_num_all, i = prepareForGSofia(non_threat_speech_train, dict_num_all, i)\n","save_graph(fin_str, \"non_threat_train_90.txt\")\n","\n","non_threat_speech_test = enterPoint(X_test[X_test['Label']==0])\n","fin_str, dict_num_all, i = prepareForGSofia(non_threat_speech_test, dict_num_all, i)\n","save_graph(fin_str, \"non_threat_test_90.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGk87RxYv-43"},"outputs":[],"source":["save_unique_id(dict_num_all)"]},{"cell_type":"markdown","source":["## Classification\n","\n","In this section graph patterns from gSofia are used to classify test examples into classes."],"metadata":{"id":"ISL6pxurJrYp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVgDaRkx3-6l"},"outputs":[],"source":["\"\"\"\n","Read files produced by gSofia\n","\"\"\"\n","def read_file(file_name):\n","    lines = []\n","    with open(file_name) as file:\n","        lines = file.read()\n","\n","    res_patters = []\n","    # res_nodes = {}\n","    cur_pat=0\n","    sup_patt_count = []\n","\n","    lines = lines.replace('#', \"\")\n","    # get all paths\n","    pattern_arr = re.split(r't  [0-9]+\\n',lines)\n","\n","    all_graphs = [] # all patters for one class\n","\n","    for pattern in pattern_arr:\n","\n","        line = pattern.split(\"\\n\")\n","        cur_edges = []\n","        cur_num = {}\n","        if len(line)<=2:\n","            continue\n","        for l in line:\n","            #vertice is found\n","            if \"v\" in l:\n","                sym = l.split(' ')\n","                # res_nodes[sym[2]]=sym[1]\n","                cur_num[sym[1]]=sym[2]\n","            # edge is found\n","            if \"e\" in l:\n","                sym = l.split(' ')\n","                # [node1, node2, edge]\n","                edg = [int(cur_num[sym[1]]), int(cur_num[sym[2]]), int(sym[3])]\n","                cur_edges.append(edg)\n","            #examples in which this path was found\n","            if \"x\" in l:\n","              sup = l.split(\" \")\n","\n","              sup_patt_count.append(len(sup[1:]))\n","              cur_pat+=1\n","\n","        all_graphs.append(cur_edges)\n","\n","    return all_graphs, sup_patt_count\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHtSFyA1AvBs"},"outputs":[],"source":["\"\"\"\n","Check if train pattern subsumes test\n","\"\"\"\n","def check_if_sumbsume(train, test):\n","  count = 0\n","  for tr in train:\n","    for t in test:\n","      if (t[0]==tr[0] or t[1]==tr[0]) and (t[0]==tr[1] or t[1]==tr[1]) and (t[2]==tr[2]):\n","        count+=1\n","\n","  if count == len(train):\n","    return True\n","  else:\n","    return False\n","\n","\"\"\"\n","Clacluate penalty fo the test example\n","\"\"\"\n","def get_pen(test, patterns_train_wrong, count_wrong_arr):\n","  res=0\n","  for i in range(len(patterns_train_wrong)):\n","    if check_if_sumbsume(patterns_train_wrong[i], test):\n","      res+=count_wrong_arr[i]\n","  if res==0:\n","    return 1\n","  else:\n","    return res\n","\n","\"\"\"\n","Clacluta both support and penalty of test example for specific class\n","\"\"\"\n","def sup_pen(test, train, patterns_train_wrong, count_right, count_wrong_arr):\n","  sup = 0\n","  pen = 1\n","\n","  if check_if_sumbsume(train, test)==False:\n","    return sup, pen\n","\n","  sup = count_right\n","  pen = get_pen(test, patterns_train_wrong, count_wrong_arr)\n","\n","  return sup, pen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZC5qzpes71kR"},"outputs":[],"source":["def agg_score(test_patters, patterns_train_right, patterns_train_wrong, count_right_arr, count_wrong_arr):\n","  L_c = sum(count_right_arr)\n","  max_contr_val_all = []\n","  max_contr_pat_all = []\n","  agg_score_all = []\n","\n","  for test in test_patters:\n","    agg_score = 0\n","    max_contr_val = -1\n","    max_contr_pat = None\n","    for i in range(len(patterns_train_right)):\n","    # for train in patterns_train_right:\n","      size_train_i = len(patterns_train_right[i])\n","      support_i, penalty_i = sup_pen(test, patterns_train_right[i], patterns_train_wrong, count_right_arr[i], count_wrong_arr)\n","      contr_i = 1/L_c * (support_i*size_train_i)/penalty_i\n","\n","      if contr_i>max_contr_val:\n","        max_contr_val = contr_i\n","        max_contr_pat = patterns_train_right[i]\n","\n","      agg_score += (support_i*size_train_i)/penalty_i\n","\n","    agg_score_all.append( (1/L_c) * agg_score)\n","    max_contr_val_all.append(max_contr_val)\n","    max_contr_pat_all.append(max_contr_pat)\n","\n","  return agg_score_all, max_contr_val_all, max_contr_pat_all"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnr1Ju2Z7hDe"},"outputs":[],"source":["t_graphs, t_sup_patt_count = read_file(\"PATTERNS_THREAT43.OUT\")\n","nt_graphs, nt_sup_patt_count = read_file(\"PATTERNS_NONTHREAT22.OUT\")\n","t_test_graph,  _ = read_file(\"threat_test.txt\")\n","nt_test_graph,  _ = read_file(\"non_threat_test.txt\")\n","\n","import pickle\n","with open('dict_vocab.txt', 'rb') as fp:\n","    dict_id = pickle.load(fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9D86tpmLKja"},"outputs":[],"source":["agg_score_threat, max_contr_val_threat, max_contr_pat_threat = agg_score(t_test_graph, t_graphs, nt_graphs, t_sup_patt_count, nt_sup_patt_count)\n","agg_score_nonthreat, max_contr_val_nonthreat, max_contr_pat_nonthreat = agg_score(nt_test_graph, nt_graphs, t_graphs, nt_sup_patt_count, t_sup_patt_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLN2hsQUQSLN"},"outputs":[],"source":["agg_score_threat_wrong, max_contr_val_threat_wrong, max_contr_pat_threat_wrong = agg_score(t_test_graph, nt_graphs, t_graphs, nt_sup_patt_count, t_sup_patt_count)\n","agg_score_nonthreat_wrong, max_contr_val_nonthreat_wrong, max_contr_pat_nonthreat_wrong = agg_score(nt_test_graph, t_graphs, nt_graphs, t_sup_patt_count, nt_sup_patt_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHmsRdKoPyeh"},"outputs":[],"source":["df_threat = pd.DataFrame(\n","    {'Agg_right': agg_score_threat,\n","     'Contr_right': max_contr_val_threat,\n","     'Patt_right': max_contr_pat_threat,\n","     'Agg_wrong': agg_score_threat_wrong,\n","     'Contr_wrong': max_contr_val_threat_wrong,\n","     'Patt_wrong': max_contr_pat_threat_wrong,\n","    })\n","df_threat['isToxic'] = np.where(df_threat['Agg_right']> df_threat['Agg_wrong'], 1, 0)\n","df_threat['cl'] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bc8KoXf4SBDC"},"outputs":[],"source":["df_nonthreat = pd.DataFrame(\n","    {'Agg_right': agg_score_nonthreat,\n","     'Contr_right': max_contr_val_nonthreat,\n","     'Patt_right': max_contr_pat_nonthreat,\n","     'Agg_wrong': agg_score_nonthreat_wrong,\n","     'Contr_wrong': max_contr_val_nonthreat_wrong,\n","     'Patt_wrong': max_contr_pat_nonthreat_wrong,\n","    })\n","df_nonthreat['isToxic'] = np.where(df_nonthreat['Agg_right']> df_nonthreat['Agg_wrong'], 0, 1)\n","df_nonthreat['cl'] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUyVgwM9VlU_"},"outputs":[],"source":["def save_image(df_row, file_name, comm, cl):\n","  dot = graphviz.Digraph(comment=comm)\n","  patt = df_row['Patt_right']\n","  print(df_row)\n","  print(X_test[X_test['Label']==cl].iloc[df_row.name].Text)\n","  print(\"------------------------\")\n","  for a in patt:\n","    dot.node(str(a[0]),list(dict_id.keys())[list(dict_id.values()).index(a[0])] )\n","    dot.node(str(a[1]),list(dict_id.keys())[list(dict_id.values()).index(a[1])])\n","    dot.edge(str(a[0]), str(a[1]), constraint='false', label=list(dict_id.keys())[list(dict_id.values()).index(a[2])])\n","  dot.render(file_name, view=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCrsMnPoWHZq"},"outputs":[],"source":["save_image(df_threat.sort_values(by=['Contr_right'], ascending=False).iloc[0], \"threat_1.png\", \"Threat intermediate\", 1)\n","save_image(df_threat.sort_values(by=['Contr_right'], ascending=False).iloc[1], \"threat_2.png\", \"Threat intermediate\", 1)\n","save_image(df_nonthreat.sort_values(by=['Contr_right'], ascending=False).iloc[0], \"nonthreat_1.png\", \"Not Threat intermediate\", 0)\n","save_image(df_nonthreat.sort_values(by=['Contr_right'], ascending=False).iloc[1], \"nonthreat_2.png\", \"Not Threat intermediate\", 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aJTDOGdZRvD"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","threat_pred_list = df_threat[(df_threat['Agg_right']!=0) & (df_threat['Agg_wrong']!=0)]['isToxic'].tolist()\n","threat_size = len(threat_pred_list)\n","non_threat_pred_list = df_nonthreat[(df_nonthreat['Agg_right']!=0) & (df_nonthreat['Agg_wrong']!=0)]['isToxic'].tolist()\n","threat_pred_list.extend(non_threat_pred_list)\n","true = [1]*threat_size\n","true.extend([0]*len(non_threat_pred_list))\n","print(classification_report(true, threat_pred_list, digits=4))"]},{"cell_type":"markdown","metadata":{"id":"3cqIP0EpP9Gm"},"source":["# Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bFaNKOWP_7x"},"outputs":[],"source":["def text_clean(text):\n","    text = emoji.demojize(text, language = \"en\")\n","    # to lower case\n","    text = text.lower()\n","    # remove mentions\n","    text = re.sub(\"@[A-Za-z0-9]+\",\"\", text)\n","    # remove hashtags\n","    text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n","    # remove links\n","    text = re.sub('https:\\/\\/\\S+', '', text)\n","    # remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n","    # remove next line\n","    text = re.sub(r'[^ \\w\\.]', '', text)\n","    # remove words containing numbers\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    # remove non latin words\n","    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n","\n","    # replace some abbreviations\n","    text = text.replace('thnx', 'thanks')\n","    text = text.replace('thx', 'thanks')\n","\n","    text = text.replace('btw', 'by the way')\n","\n","    text = text.replace('pls', 'please')\n","    text = text.replace('plz', 'please')\n","\n","    text = text.replace('imho', 'in my humble opinion')\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":662,"status":"ok","timestamp":1735287178485,"user":{"displayName":"Анна Назарова","userId":"17138035556691584905"},"user_tz":-180},"id":"Z4Xy1sknQZtz","outputId":"ff735b22-d72c-4e0b-fcc0-9defaa6d1494"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df\",\n  \"rows\": 1198,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1186,\n        \"samples\": [\n          \"exposed mother outside world opened door whole pakistan\",\n          \"make poor uneducated tax amort much bring wealth\",\n          \"bilawal set good example harami earning haram\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df"},"text/html":["\n","  <div id=\"df-a0885d21-79bc-4857-a407-985ce9eacbb6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>mustaches cut use much power possible people u...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>people karachi catch robbers burn alive whenev...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>coward tries disqualify imran khan whether cow...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ready understand tomorrow kind revenge action ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>waiting four names mouth khan made country hon...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0885d21-79bc-4857-a407-985ce9eacbb6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a0885d21-79bc-4857-a407-985ce9eacbb6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a0885d21-79bc-4857-a407-985ce9eacbb6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-98ff8eb0-7532-4da7-857c-a62cd8724c78\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98ff8eb0-7532-4da7-857c-a62cd8724c78')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-98ff8eb0-7532-4da7-857c-a62cd8724c78 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                                                Text  Label\n","0  mustaches cut use much power possible people u...      1\n","1  people karachi catch robbers burn alive whenev...      1\n","2  coward tries disqualify imran khan whether cow...      1\n","3  ready understand tomorrow kind revenge action ...      1\n","4  waiting four names mouth khan made country hon...      1"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df['Text'] = df['Text'].apply(lambda x: text_clean(x))\n","df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_eng)]))\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dzDBLFfQzpv"},"outputs":[],"source":["\"\"\"\n","Gnerating 90:10 split for train:test. The \"is_unigram\" is option for the Dataframes that have extra attributes\n","\"\"\"\n","def generate_split(df, is_unigram=True, test_size = 0.1):\n","  split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n","  X_train, y_train, X_test, y_test = None, None, None, None\n","  if is_unigram:\n","   X = df.drop(columns = ['Text', 'Label']).reset_index(drop=True)\n","  else:\n","    X = df.drop(columns = ['Label']).reset_index(drop=True)\n","  y = df['Label'].reset_index(drop=True)\n","  for i, (train_index, test_index) in enumerate(split.split(X, y)):\n","          X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n","          X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n","          y_train = y_train\n","          y_test = y_test\n","\n","          X_train = X_train\n","          X_test = X_test\n","\n","  return X_train, y_train, X_test, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLrCDD22pIrX"},"outputs":[],"source":["def print_res(X_train, y_train, X_test, y_test):\n","  sgd = SGDClassifier(loss=\"perceptron\", eta0=1.0, learning_rate=\"constant\", penalty=None, random_state=seed_val)\n","  sgd.fit(X_train, y_train)\n","  y_pred_sgd= sgd.predict(X_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_sgd).ravel()\n","  print(\"SGDClassifier\")\n","  print(classification_report(y_test, y_pred_sgd, digits=4))\n","  print(tp, tn, fp, fn)\n","  print('__________________')\n","\n","\n","  knn = KNeighborsClassifier()\n","  knn.fit(X_train, y_train)\n","  y_pred_knn = knn.predict(X_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()\n","  print(\"KNeighborsClassifier\")\n","  print(classification_report(y_test, y_pred_knn, digits=4))\n","  print(tp, tn, fp, fn)\n","  print('__________________')\n","\n","  nb = GaussianNB()\n","  nb.fit(X_train, y_train)\n","  y_pred_nb = nb.predict(X_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_nb).ravel()\n","  print(\"GaussianNB\")\n","  print(classification_report(y_test, y_pred_nb, digits=4))\n","  print(tp, tn, fp, fn)\n","  print('__________________')"]},{"cell_type":"markdown","metadata":{"id":"IKRgPpRGREk4"},"source":["## Word n-gram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDkDYqwzRWFX"},"outputs":[],"source":["def flat_list(unflat_list):\n","    flatted = [item for sublist in unflat_list for item in sublist]\n","    return flatted\n","\"\"\"\n","Turning DataFrame of Text into list, so we can make unigrams\n","\"\"\"\n","def to_list(df, attribute):\n","    df_transcription = df[[attribute]]\n","    unflat_list_transcription = df_transcription.values.tolist()\n","    flat_list_transcription = flat_list(unflat_list_transcription)\n","    return flat_list_transcription\n","\n","def generate_n_gram_features(flat_list_transcription, df):\n","    vectorizer = CountVectorizer(ngram_range=(1,1))#unigram\n","    X = vectorizer.fit_transform(flat_list_transcription)\n","    count_vect_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names_out())\n","    df = pd.concat([df, count_vect_df], axis=1)\n","    return X, df\n","\n","temp, df_features = generate_n_gram_features(to_list(df, 'Text'), df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ViI8ZfeR3fa"},"outputs":[],"source":["X_train, y_train, X_test, y_test = generate_split(df_features, test_size = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJxzTAkYqW9H"},"outputs":[],"source":["print_res(X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"nRRI4AlZR_EY"},"source":["## Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkEt6LA1SSSB"},"outputs":[],"source":["X_train, y_train, X_test, y_test = generate_split(df, is_unigram=False, test_size = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEOUzVxbSWfE"},"outputs":[],"source":["def vectorize(sentence):\n","    words = sentence.split()\n","    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n","    if len(words_vecs) == 0:\n","        return np.zeros(100)\n","    words_vecs = np.array(words_vecs)\n","    return words_vecs.mean(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2EBAAJFSdXw"},"outputs":[],"source":["words = X_train['Text'].apply(lambda x: x.split(\" \"))\n","\n","w2v_model = gensim.models.Word2Vec(words, vector_size = 100, sg=1, window=1)\n","X_train_features = X_train['Text'].apply(vectorize)\n","X_test_features = X_test['Text'].apply(vectorize)\n","\n","\n","feature = [x for x in X_train_features.transpose()]\n","X_train_features = np.asarray(feature)\n","\n","feature = [x for x in X_test_features.transpose()]\n","X_test_features = np.asarray(feature)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXH0NsPZqa05"},"outputs":[],"source":["print_res(X_train_features, y_train, X_test_features, y_test)"]},{"cell_type":"markdown","metadata":{"id":"PI2pllizT4a9"},"source":["## FastText"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoQ8zv6NT6KF"},"outputs":[],"source":["X_train, y_train, X_test, y_test = generate_split(df, is_unigram=False, test_size = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqqBFrlJUCRF"},"outputs":[],"source":["corpus=[]\n","for i in X_train['Text'].values:\n","    corpus.append(str(i).split(\" \"))\n","model = gensim.models.FastText(corpus, vector_size=100, workers=4,window=5)\n","\n","def vectorize_fattext(sentence):\n","    words = sentence.split()\n","    words_vecs = [model.wv[word] for word in words if word in model.wv]\n","    if len(words_vecs) == 0:\n","        return np.zeros(100)\n","    words_vecs = np.array(words_vecs)\n","    return words_vecs.mean(axis=0)\n","\n","def get_features(X_train, X_test):\n","  X_train_features = X_train['Text'].apply(vectorize_fattext)\n","  X_test_features = X_test['Text'].apply(vectorize_fattext)\n","\n","\n","  feature = [x for x in X_train_features.transpose()]\n","  X_train_features = np.asarray(feature)\n","\n","  feature = [x for x in X_test_features.transpose()]\n","  X_test_features = np.asarray(feature)\n","\n","  return X_train_features, X_test_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BzHg-CLjrDmm"},"outputs":[],"source":["X_train_features, X_test_features = get_features(X_train, X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqp492-5rInP"},"outputs":[],"source":["print_res(X_train_features, y_train, X_test_features, y_test)"]},{"cell_type":"markdown","metadata":{"id":"EfwfRhrzSxNw"},"source":["## LIWC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cmrzDM0TLxw"},"outputs":[],"source":["parse, category_names = liwc.load_token_parser('LIWC2007_English080730.dic')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CcEVcWGTeeV"},"outputs":[],"source":["def tokenize(text):\n","    for match in re.finditer(r'\\w+', text, re.UNICODE):\n","        yield match.group(0)\n","\n","def full_arr(category_names, counts, full_len):\n","  res = []\n","  for c in category_names:\n","    res.append(counts[c])\n","\n","  return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkyZPEEkTmUC"},"outputs":[],"source":["res = []\n","for i in range(len(df)):\n","\n","  sen = df.loc[i].Text\n","  help = [sen]\n","\n","  tokens = tokenize(sen)\n","  counts = Counter(category for token in tokens for category in parse(token))\n","\n","  help.extend(full_arr(category_names, counts, len(df.loc[i].Text.split(\" \"))))\n","  help.append(df.loc[i].Label)\n","\n","  res.append(help)\n","columns_name = [\"Text\"]\n","columns_name.extend(category_names)\n","columns_name.append('Label')\n","\n","df_new = pd.DataFrame(data = res, columns = columns_name )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDSTJ78bT0eu"},"outputs":[],"source":["X_train, y_train, X_test, y_test = generate_split(df_new, is_unigram=True, test_size = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9opNDoJRrL5g"},"outputs":[],"source":["print_res(X_train, y_train, X_test, y_test)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPL2GL5BAlbeedo4F7C5S5N"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}